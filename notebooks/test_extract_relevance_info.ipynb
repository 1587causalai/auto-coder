{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing extract_relevance_info_from_docs_with_conversation Function\n",
    "\n",
    "This notebook demonstrates how to use and test the `extract_relevance_info_from_docs_with_conversation` function from the ByzerLLM library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-08-28 15:26:16.635\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mbyzerllm.utils.connect_ray\u001b[0m:\u001b[36mconnect_cluster\u001b[0m:\u001b[36m48\u001b[0m - \u001b[1mJDK 21 will be used (/Users/allwefantasy/.auto-coder/jdk-21.0.2.jdk/Contents/Home)...\u001b[0m\n",
      "2024-08-28 15:26:16,663\tINFO worker.py:1564 -- Connecting to existing Ray cluster at address: 127.0.0.1:6379...\n",
      "2024-08-28 15:26:16,695\tINFO worker.py:1740 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m The autoscaler failed with the following error:\n",
      "Terminated with signal 15\n",
      "  File \"/opt/miniconda3/envs/byzerllm/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.py\", line 709, in <module>\n",
      "    monitor.run()\n",
      "  File \"/opt/miniconda3/envs/byzerllm/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.py\", line 584, in run\n",
      "    self._run()\n",
      "  File \"/opt/miniconda3/envs/byzerllm/lib/python3.10/site-packages/ray/autoscaler/_private/monitor.py\", line 438, in _run\n",
      "    time.sleep(AUTOSCALER_UPDATE_INTERVAL_S)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import byzerllm\n",
    "from byzerllm import ByzerLLM\n",
    "\n",
    "# Initialize ByzerLLM\n",
    "llm = ByzerLLM.from_default_model(\"deepseek_chat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Test Data\n",
    "\n",
    "Let's create some sample documents and a conversation history to test our function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample documents\n",
    "documents = [\n",
    "    \"The capital of France is Paris. It is known for its beautiful architecture and cuisine.\",\n",
    "    \"Python is a popular programming language used in data science and web development.\",\n",
    "    \"The Great Wall of China is one of the most famous landmarks in the world.\"\n",
    "]\n",
    "\n",
    "# Sample conversation history\n",
    "conversations = [\n",
    "    {\"role\": \"user\", \"content\": \"Tell me about France.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"France is a country in Western Europe known for its rich history and culture.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What's the capital of France?\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and Test the Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "@byzerllm.prompt()\n",
    "def extract_relevance_info_from_docs_with_conversation(conversations, documents) -> str:\n",
    "    \"\"\"\n",
    "    使用以下文档和对话历史来提取相关信息。\n",
    "\n",
    "    文档：\n",
    "    {% for doc in documents %}\n",
    "    {{ doc }}\n",
    "    {% endfor %}\n",
    "\n",
    "    对话历史：\n",
    "    {% for msg in conversations %}\n",
    "    <{{ msg.role }}>: {{ msg.content }}\n",
    "    {% endfor %}\n",
    "\n",
    "    请根据提供的文档内容、用户对话历史以及最后一个问题，提取并总结文档中与问题相关的重要信息。\n",
    "    如果文档中没有相关信息，请回复\"该文档中没有与问题相关的信息\"。\n",
    "    提取的信息尽量保持和原文中的一样，并且只输出这些信息。\n",
    "    \"\"\"\n",
    "\n",
    "# Test the function\n",
    "result = extract_relevance_info_from_docs_with_conversation.with_llm(llm).run(conversations=conversations, documents=documents)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze the Results\n",
    "\n",
    "Let's analyze the output of our function to see if it correctly extracted the relevant information from the documents based on the conversation history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Extracted relevant information:\")\n",
    "print(result)\n",
    "\n",
    "print(\"\\nDoes the extracted information answer the user's last question?\")\n",
    "print(\"User's last question:\", conversations[-1]['content'])\n",
    "print(\"Relevant information found:\", \"Yes\" if \"Paris\" in result else \"No\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with Different Scenarios\n",
    "\n",
    "Let's test our function with different scenarios to ensure it works correctly in various situations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 1: Question about a topic not in the documents\n",
    "conversations_scenario1 = [\n",
    "    {\"role\": \"user\", \"content\": \"What's the capital of Germany?\"},\n",
    "]\n",
    "\n",
    "result_scenario1 = extract_relevance_info_from_docs_with_conversation.with_llm(llm).run(conversations=conversations_scenario1, documents=documents)\n",
    "print(\"Scenario 1 Result:\")\n",
    "print(result_scenario1)\n",
    "\n",
    "# Scenario 2: Question about a topic in the documents, but not related to France\n",
    "conversations_scenario2 = [\n",
    "    {\"role\": \"user\", \"content\": \"Tell me about Python programming.\"},\n",
    "]\n",
    "\n",
    "result_scenario2 = extract_relevance_info_from_docs_with_conversation.with_llm(llm).run(conversations=conversations_scenario2, documents=documents)\n",
    "print(\"\\nScenario 2 Result:\")\n",
    "print(result_scenario2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've tested the `extract_relevance_info_from_docs_with_conversation` function with various scenarios. The function should effectively extract relevant information from the provided documents based on the conversation history and the user's last question. \n",
    "\n",
    "By analyzing the results, we can see how well the function performs in different situations, such as when the relevant information is present in the documents or when it's not available."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
