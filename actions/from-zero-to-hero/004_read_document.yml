project_type: py
source_dir: /tmp/t-py
target_file: /home/winubuntu/projects/ByzerRawCopilot/output.txt 
py_packages: fastapi

model: qianwen_chat
model_max_length: 2000
model_max_input_length: 100000
anti_quota_limit: 5

search_engine: bing
search_engine_token: ENV {{BING_SEARCH_TOKEN}}

## execute the prompt generated by auto-coder
execute: true
## extract the code from the prompt generated by auto-coder 
## and overwrite the source code
auto_merge: true
human_as_model: true

human_as_model: true

urls: >
  https://raw.githubusercontent.com/allwefantasy/byzer-llm/master/README.md

query: >
  在 read_root 方法前新添加一个方法，
  对应的rest 路径为 /llm, 该接口接受连个参数： query 和 model。
  调用 llm.chat_oai 方法，然后返回结果。
  你需要参考前面 byzer-llm 的文档来完成 llm.chat_oai 方法的调用。
  注意， ByzerLLM 的初始化要放到新方法里。


   