source_dir: /home/winubuntu/projects/ByzerRawCopilot 
target_file: /home/winubuntu/projects/ByzerRawCopilot/output.txt 
query: |
  把下面这段代码抽取出一个函数，变化的部分是 response_class, 参数部分应该是conversations。
  t = self.llm.chat_oai(conversations=conversations,response_class=ExecuteStep)        
  max_steps = 30
  total_steps = max_steps
  current_step = 0
  
  if not t[0].value:
      total_steps = t[0].value.total_steps
      if total_steps == 1:
          current_step = 1

  while current_step < total_steps and max_steps>0 and t[0].value:                             
      total_steps = t[0].value.total_steps                
      final_v.steps.append(t[0].value)
      conversations.append({
          "role":"assistant",
          "content":t[0].response.output
      })
      print(f"{conversations[-1]['role']}: {conversations[-1]['content']}\n",flush=True)

      conversations.append({
          "role":"user",
          "content":"继续"
      })            
      print(f"{conversations[-1]['role']}: {conversations[-1]['content']}\n",flush=True)

      t = self.llm.chat_oai(conversations=conversations,response_class=ExecuteStep)
      max_steps -= 1  
      current_step += 1 
      time.sleep(args.anti_quota_limit) 